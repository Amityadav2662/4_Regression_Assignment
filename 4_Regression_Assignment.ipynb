{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c21fcd-e614-4957-94b6-9c7bc90f3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Ans.\n",
    "Lasso Regression, also known as Least Absolute Shrinkage and Selection Operator, is a regression technique\n",
    "used for both modeling and feature selection. Here's how it differs:\n",
    "1. Regularization: It employs L1 regularization (penalty based on the sum of absolute values of coefficients)\n",
    "compared to L2 used in Ridge Regression.\n",
    "2. Coefficient Shrinking: Unlike Ridge Regression, which shrinks coefficients towards zero, Lasso can set them\n",
    "to zero, effectively removing those features from the model.\n",
    "3. Feature Selection: This ability makes Lasso ideal for identifying relevant features and reducing model \n",
    "complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be65d7-248d-4046-af79-d358719644a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "Ans.\n",
    "The main advantage of using Lasso Regression for feature selection is its ability to automatically identify and\n",
    "remove irrelevant features from the model. This can lead to:\n",
    "1. Improved model interpretability: With fewer features, it's easier to understand the relationship between the \n",
    "remaining features and the target variable.\n",
    "2. Reduced overfitting: By removing irrelevant features, Lasso can help prevent the model from overfitting to the\n",
    "training data, leading to better performance on unseen data.\n",
    "3. Improved model performance: In some cases, removing irrelevant features can actually improve the model's ability\n",
    "to predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606b694-109f-430a-a36c-e330d5e24d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "Ans.\n",
    "Due to coefficient shrinkage, interpreting individual values in a Lasso model can be challenging. However, you can\n",
    "still gain insights:\n",
    "1. Non-zero coefficients: Features with non-zero coefficients are considered important for the model's prediction.\n",
    "2. Sign of coefficients: The sign (+/-) retains its meaning, indicating the positive or negative relationship with\n",
    "the target variable.\n",
    "3. Relative importance: Compare the magnitude of non-zero coefficients to gauge their relative contribution.\n",
    "4. Caution: Remember, individual coefficients don't represent the full feature impact due to shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e00dd-c329-49fc-b745-a87ee2efe546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "# model's performance?\n",
    "Ans.\n",
    "In Lasso Regression, the primary tuning parameter to adjust is lambda (λ), which controls the strength of the L1 \n",
    "penalty. This penalty term encourages sparsity by shrinking the coefficients towards zero, potentially setting some\n",
    "to zero completely. Tuning lambda affects the model's performance in several ways:\n",
    "\n",
    "Impact on sparsity:\n",
    "Higher λ: Shrinks coefficients more aggressively, leading to fewer features in the model. This simplifies the model\n",
    "and reduces overfitting risk, but can also increase bias if too high.\n",
    "Lower λ: Relaxes the penalty, allowing more features to contribute. This can offer more flexibility and potentially\n",
    "capture complex relationships, but increases model complexity and risk of overfitting.\n",
    "\n",
    "Impact on performance:\n",
    "1. Bias-Variance trade-off: By adjusting λ, you essentially trade off bias and variance. Lower λ leads to higher \n",
    "variance (more flexible but susceptible to overfitting), while higher λ introduces more bias (simpler but less \n",
    "accurate). Finding the optimal λ balances these competing factors.\n",
    "2. Generalization: Choosing the right λ can lead to better generalization, meaning the model performs well on unseen\n",
    "data. Too low, and the model might overfit the training data. Too high, and it might underfit and miss important\n",
    "information.\n",
    "3. Interpretability: With higher λ, more features get eliminated, leading to a sparser and more interpretable model.\n",
    "It's easier to understand which features are important and how they affect the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca02c8f-dfc5-499b-bfb1-91d53496eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Ans.\n",
    "No, Lasso Regression itself cannot directly handle non-linear regression problems. It's designed for linear \n",
    "relationships between features and the target variable. However, there are ways to leverage its strengths for\n",
    "non-linear scenarios.\n",
    "\n",
    "1. Polynomial Expansion: Transform your features by creating new ones based on their powers and interactions. This\n",
    "allows you to capture non-linear relationships using linear combinations of features.\n",
    "2. Basis Functions: Choose functions like splines or wavelets that can represent non-linearity. Then, use these \n",
    "functions as additional features in your Lasso Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268913b4-1851-41c8-b942-9d93c53ae002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "Ans.\n",
    "Both Ridge Regression and Lasso Regression are regularization techniques used to improve the performance \n",
    "of linear regression models by addressing overfitting, but they differ in their approach and impact:\n",
    "1. Penalization:\n",
    "Ridge Regression: Uses L2 regularization, penalizing the sum of squared coefficients. This shrinks all \n",
    "coefficients towards zero, but doesn't necessarily set them to zero.\n",
    "Lasso Regression: Uses L1 regularization, penalizing the sum of absolute values of coefficients. This can \n",
    "shrink some coefficients to zero, effectively removing those features from the model (feature selection).\n",
    "\n",
    "2. Sparsity:\n",
    "Ridge Regression: Creates denser models with most coefficients non-zero, potentially leading to higher\n",
    "variance.\n",
    "Lasso Regression: Creates sparser models with many coefficients set to zero, reducing variance and \n",
    "improving interpretability.\n",
    "\n",
    "3. Bias-Variance Trade-off:\n",
    "Ridge Regression: Introduces more bias than Lasso to reduce variance.\n",
    "Lasso Regression: Introduces less bias than Ridge but can have higher variance due to fewer features.\n",
    "\n",
    "4. Overfitting:\n",
    "Ridge Regression: Effective in preventing overfitting by reducing coefficient magnitudes.\n",
    "Lasso Regression: Effective in preventing overfitting and selecting relevant features simultaneously.\n",
    "\n",
    "5. Multicollinearity:\n",
    "Ridge Regression: More robust to multicollinearity by shrinking correlated features' coefficients together.\n",
    "Lasso Regression: Can handle multicollinearity by potentially eliminating correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3f59a-4e47-4a13-b86e-b8e7eee1cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Ans.\n",
    "Yes, Lasso Regression can effectively handle multicollinearity (correlated features) due to its inherent\n",
    "feature selection property:\n",
    "1. L1 penalty: Shrinks coefficients towards zero, potentially setting some to zero completely.\n",
    "2. Elimination: When features are highly correlated, Lasso tends to eliminate one or more, reducing their\n",
    "collective impact on the model.\n",
    "3. Reduced variance: By addressing multicollinearity, Lasso helps stabilize coefficient estimates and\n",
    "reduces variance, leading to better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0989e86-d8f4-48bb-9e67-3ea80b301eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "Ans.\n",
    "Choosing the optimal lambda in Lasso Regression is crucial, but there's no \"magic number.\" Here are some \n",
    "key strategies to find the sweet spot:\n",
    "1. Cross-validation: Divide your data into folds, train models with different lambda values on each fold,\n",
    "and choose the lambda with the best performance (e.g., accuracy) on unseen data.\n",
    "2. Information criteria: Use metrics like AIC or BIC that penalize complex models (higher lambda) to guide\n",
    "lambda selection.\n",
    "3. Grid search/Random search: Try various lambda values, evaluate performance on your data, and choose the\n",
    "best one. Consider random search for efficiency with large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
